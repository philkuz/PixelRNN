{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TensorFlow to Generate Images with PixelRNNs\n",
    "Let's start by importing all the necessary dependencies.\n",
    "\n",
    "Note that, as mentioned in the article, this notebook will use a faster, simpler version of the PixelRNN architecture called PixelCNN, which just relies on a series of masked convolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "\n",
    "from utils import *\n",
    "from ops import *\n",
    "from statistic import Statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the parameters\n",
    "Here we set all the paramaters for the PixelRNN model. These include the model hyperparameters, some dataset properties, and debugging information. We also set up the random seeds for Tensorflow and Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hyperparams = {# network\n",
    "    \"model\" : \"pixel_cnn\", # name of model [pixel_rnn, pixel_cnn]\n",
    "    \"batch_size\" : 100, # size of a batch\n",
    "    \"hidden_dims\" : 16, # dimesion of hidden states of LSTM or Conv layers\n",
    "    \"recurrent_length\" : 7, # the length of LSTM or Conv layers\n",
    "    \"out_hidden_dims\" : 32, # dimesion of hidden states of output Conv layers\n",
    "    \"out_recurrent_length\" : 2, # the length of output Conv layers\n",
    "    \"use_residual\" : False, # whether to use residual connections or not\n",
    "    \"use_dynamic_rnn\" : False, # whether to use dynamic_rnn or not\n",
    "\n",
    "    # training\n",
    "    \"max_epoch\" : 20, # # of step in an epoch\n",
    "    \"test_step\" : 10, # # of step to test a model\n",
    "    \"save_step\" : 5, # # of step to save a model\n",
    "    \"learning_rate\" : 1e-3, # learning rate\n",
    "    \"grad_clip\" : 1, # value of gradient to be used for clipping\n",
    "    \"use_gpu\" : True, # whether to use gpu for training\n",
    "\n",
    "    # data\n",
    "    \"data\" : \"mnist\", # name of dataset \n",
    "    \"data_dir\" : \"MNIST-data\", # name of data directory\n",
    "    \"sample_dir\" : \"samples\", # name of sample directory\n",
    "\n",
    "    # Debug\n",
    "    \"is_train\" : True, # training or testing\n",
    "    \"display\" : False, # whether to display the training results or not\n",
    "    \"random_seed\" :  123 # random seed for python\n",
    "}\n",
    "p = dotdict(hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if \"random_seed\" in p:\n",
    "    tf.set_random_seed(p.random_seed)\n",
    "    np.random.seed(p.random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the dataset\n",
    "\n",
    "Next, let's load the MNIST image data from which we will train our model. [MNIST](http://yann.lecun.com/exdb/mnist/) is a dataset of handwritten digits, commonly used as a benchmark for machine learning tasks. We will try to generate images that look similar to those contained in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO add hyperparams to model saving\n",
    "model_dir = setup_model_saving(p.model, p.data, hyperparams)\n",
    "DATA_DIR = p.data_dir\n",
    "SAMPLE_DIR = os.path.join(model_dir, p.sample_dir)\n",
    "\n",
    "check_and_create_dir(DATA_DIR)\n",
    "check_and_create_dir(SAMPLE_DIR)\n",
    "\n",
    "# prepare dataset\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(DATA_DIR, one_hot=True)\n",
    "\n",
    "next_train_batch = lambda x: mnist.train.next_batch(x)[0]\n",
    "next_test_batch = lambda x: mnist.test.next_batch(x)[0]\n",
    "\n",
    "height, width, channel = 28, 28, 1\n",
    "\n",
    "train_step_per_epoch = mnist.train.num_examples // p.batch_size\n",
    "test_step_per_epoch = mnist.test.num_examples // p.batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up network\n",
    "\n",
    "Let's construct the PixelCNN model. First, we set up input placeholder. We'll be feeding batches of training images into the model through this.\n",
    "\n",
    "Next, we construct the masked convolutional layers. You can find the implementation of this masking procedure in ```ops.py```. These layers apply a series of convolutions to the image, where each filter is masked to only account for pixels in the region of interest. These are the pixels above and to the left of the pixel in the center of the mask, which follows the PixelRNN generative model assumptions. Also of note is that the receptive field of the PixelCNN model grows linearly with the depth of these convolutional stacks.\n",
    "\n",
    "After the convolutions, we apply a sigmoid activation to arrive at an estimate for the grayscale intensity for each pixel in the output image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pixelRNN(height, width, channel, params):\n",
    "    \"\"\"\n",
    "    Args\n",
    "    height, width, channel - the dimensions of the input\n",
    "    params -- the hyperparameters of the network\n",
    "    \"\"\"\n",
    "    input_shape = [None, height, width, channel] if params.use_gpu else [None, channel, height, width]\n",
    "    inputs = tf.placeholder(tf.float32, input_shape)\n",
    "\n",
    "    # input of main convolutional layers\n",
    "    scope = \"conv_inputs\"\n",
    "    conv_inputs = conv2d(inputs, params.hidden_dims, [7, 7], \"A\", scope=scope)\n",
    "    \n",
    "    # main convolutions layers\n",
    "    last_hid = conv_inputs\n",
    "    for idx in range(params.recurrent_length):\n",
    "        scope = 'CONV%d' % idx\n",
    "        last_hid = conv2d(last_hid, 3, [1, 1], \"B\", scope=scope)\n",
    "        print(\"Building %s\" % scope)\n",
    "\n",
    "    # output convolutional layers\n",
    "    for idx in range(params.out_recurrent_length):\n",
    "        scope = 'CONV_OUT%d' % idx\n",
    "        last_hid = tf.nn.relu(conv2d(last_hid, params.out_hidden_dims, [1, 1], \"B\", scope=scope))\n",
    "        print(\"Building %s\" % scope)\n",
    "\n",
    "    conv2d_out_logits = conv2d(last_hid, 1, [1, 1], \"B\", scope='conv2d_out_logits')\n",
    "    output = tf.nn.sigmoid(conv2d_out_logits)\n",
    "    return inputs, output, conv2d_out_logits\n",
    "\n",
    "inputs, output, conv2d_out_logits = pixelRNN(height, width, channel, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "\n",
    "Now, let's train the model. To do so, we will minimize the cross entropy loss using an RMSPropOptimizer. We also clip the gradients to help deal with potential exploding gradient problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=conv2d_out_logits, labels=inputs, name='loss'))\n",
    "\n",
    "optimizer = tf.train.RMSPropOptimizer(p.learning_rate)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "\n",
    "new_grads_and_vars = \\\n",
    "    [(tf.clip_by_value(gv[0], -p.grad_clip, p.grad_clip), gv[1]) for gv in grads_and_vars]\n",
    "optim = optimizer.apply_gradients(new_grads_and_vars)\n",
    " \n",
    "# show_all_variables()\n",
    "print(\"Building %s finished!\" % p.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image generation\n",
    "To generate an image, we predict a single pixel at a time. Once we generate a pixel, the next prediction will use the previous pixels to generate the next pixel intensity using the masked convolutions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(sess, images, inputs, output):\n",
    "    return sess.run(output, {inputs: images})\n",
    "def generate_occlusions(sess, height, width, inputs, output):\n",
    "    samples = occlude(images, height, width)\n",
    "    starting_position = [0,height//2]\n",
    "    for i in range(starting_position[1], height):\n",
    "        for j in range(starting_position[0], width):\n",
    "            next_sample = binarize(predict(sess, samples, inputs, output))\n",
    "            samples[:, i, j] = next_sample[:, i, j]\n",
    "    return samples\n",
    "\n",
    "def generate(sess, height, width, inputs, output):\n",
    "    samples = np.zeros((100, height, width, 1), dtype='float32')\n",
    "\n",
    "    for i in range(height):\n",
    "        for j in range(width):\n",
    "            next_sample = binarize(predict(sess, samples, inputs, output))\n",
    "            samples[:, i, j] = next_sample[:, i, j]\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Now we start the actual training process. We initialize Tensorflow and load a Statistics class to keep track of the model and score statistics as we train.\n",
    "\n",
    "Then, we go through our training epochs, generating a series of images, evaluating the loss and optimizing accordingly. Every epoch we also test the model by producing sample images and evaluating the performance on some test data. Finally, once training is finished, we generate the images to display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# with tf.Session() as sess:\n",
    "sess = tf.Session()\n",
    "stat = Statistic(sess, p.data, model_dir, tf.trainable_variables(), p.test_step)\n",
    "stat.load_model()\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess.run(init)\n",
    "stat.start()\n",
    "print(\"Start training\")\n",
    "\n",
    "initial_step = stat.get_t() if stat else 0\n",
    "# iterator = trange(p.max_epoch, ncols=70, initial=initial_step)\n",
    "iterator = tqdm(range(p.max_epoch))\n",
    "\n",
    "for epoch in iterator:\n",
    "    # print('Start epoch')\n",
    "    # 1. train\n",
    "    total_train_costs = []\n",
    "    for idx in range(train_step_per_epoch):\n",
    "        images = binarize(next_train_batch(p.batch_size)) \\\n",
    "            .reshape([p.batch_size, height, width, channel])\n",
    "\n",
    "        _, cost = sess.run([optim, loss], feed_dict={ inputs: images })\n",
    "        total_train_costs.append(cost)\n",
    "    # print('Start testing')\n",
    "    # 2. test\n",
    "    total_test_costs = []\n",
    "    for idx in range(test_step_per_epoch):\n",
    "        images = binarize(next_test_batch(p.batch_size)) \\\n",
    "            .reshape([p.batch_size, height, width, channel])\n",
    "\n",
    "        cost = sess.run(loss, feed_dict={ inputs : images })\n",
    "        total_test_costs.append(cost)\n",
    "\n",
    "    avg_train_cost, avg_test_cost = np.mean(total_train_costs), np.mean(total_test_costs)\n",
    "\n",
    "    stat.on_step(avg_train_cost, avg_test_cost)\n",
    "    # print('Start generation')\n",
    "    # 3. generate samples\n",
    "    samples = generate_occlusions(sess, height, width, inputs, output)\n",
    "    path = save_images(samples, height, width, 10, 10, \n",
    "        directory=SAMPLE_DIR, prefix=\"epoch_%s\" % epoch)\n",
    "    iterator.set_description(\"train loss: %.3f, test loss: %.3f\" % (avg_train_cost, avg_test_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with tf.Session() as sess:\n",
    "samples = generate_occlusions(sess, height, width, inputs, output)\n",
    "save_images(samples, height, width, 10, 10, directory=SAMPLE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "fname = save_images(samples, height, width, 10, 10, directory=SAMPLE_DIR)\n",
    "Image(filename=fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we are done, we close our Tensorflow session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
